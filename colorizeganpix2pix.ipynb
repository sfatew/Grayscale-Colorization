{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xrFwfLfkLgTe"
   },
   "source": [
    "# Image Colorization with U-Net and GAN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:08:21.148536Z",
     "iopub.status.busy": "2025-05-30T10:08:21.148115Z",
     "iopub.status.idle": "2025-05-30T10:08:28.895944Z",
     "shell.execute_reply": "2025-05-30T10:08:28.894545Z",
     "shell.execute_reply.started": "2025-05-30T10:08:21.148501Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install -U albumentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZyiqOWCMLgTw"
   },
   "source": [
    "# Loading Image Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:08:28.898220Z",
     "iopub.status.busy": "2025-05-30T10:08:28.897904Z",
     "iopub.status.idle": "2025-05-30T10:08:41.917018Z",
     "shell.execute_reply": "2025-05-30T10:08:41.915788Z",
     "shell.execute_reply.started": "2025-05-30T10:08:28.898190Z"
    },
    "id": "4dgGWkD5LgTx",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.color import rgb2lab, lab2rgb\n",
    "\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:08:41.918653Z",
     "iopub.status.busy": "2025-05-30T10:08:41.918145Z",
     "iopub.status.idle": "2025-05-30T10:08:41.924837Z",
     "shell.execute_reply": "2025-05-30T10:08:41.922277Z",
     "shell.execute_reply.started": "2025-05-30T10:08:41.918624Z"
    },
    "id": "RuAkEfd7LgT1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install fastai==2.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UTrAsLbjLgT1"
   },
   "source": [
    "The following will download about 20,000 images from COCO dataset. Notice that **we are going to use only 8000 of them** for training. Also you can use any other dataset like ImageNet as long as it contains various scenes and locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:08:41.927710Z",
     "iopub.status.busy": "2025-05-30T10:08:41.927304Z",
     "iopub.status.idle": "2025-05-30T10:08:41.953741Z",
     "shell.execute_reply": "2025-05-30T10:08:41.952653Z",
     "shell.execute_reply.started": "2025-05-30T10:08:41.927677Z"
    },
    "id": "GLst-tGnLgT1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from fastai.data.external import untar_data, URLs\n",
    "# coco_path = untar_data(URLs.COCO_SAMPLE)\n",
    "# coco_path = str(coco_path) + \"/train_sample\"\n",
    "# use_colab = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:08:41.955653Z",
     "iopub.status.busy": "2025-05-30T10:08:41.955282Z",
     "iopub.status.idle": "2025-05-30T10:08:41.982388Z",
     "shell.execute_reply": "2025-05-30T10:08:41.980977Z",
     "shell.execute_reply.started": "2025-05-30T10:08:41.955627Z"
    },
    "id": "8iNGEbpZLgT1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "data_set_root='/kaggle/input/coco-2017-dataset/coco2017'\n",
    "train_set ='train2017'\n",
    "validation_set ='val2017'\n",
    "test_set = 'test2017'\n",
    "\n",
    "train_path = os.path.join(data_set_root, train_set)\n",
    "\n",
    "val_path = os.path.join(data_set_root, validation_set)\n",
    "\n",
    "test_path = os.path.join(data_set_root, test_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:08:41.984621Z",
     "iopub.status.busy": "2025-05-30T10:08:41.984146Z",
     "iopub.status.idle": "2025-05-30T10:18:42.255048Z",
     "shell.execute_reply": "2025-05-30T10:18:42.253965Z",
     "shell.execute_reply.started": "2025-05-30T10:08:41.984585Z"
    },
    "id": "qu4pJxAYLgT1",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_image_path = list(Path(train_path).rglob(\"*.*\"))\n",
    "val_image_path = list(Path(val_path).rglob(\"*.*\"))\n",
    "test_image_path = list(Path(test_path).rglob(\"*.*\"))\n",
    "\n",
    "print(len(train_image_path), len(val_image_path), len(test_image_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:18:42.256791Z",
     "iopub.status.busy": "2025-05-30T10:18:42.256386Z",
     "iopub.status.idle": "2025-05-30T10:18:42.714919Z",
     "shell.execute_reply": "2025-05-30T10:18:42.713305Z",
     "shell.execute_reply.started": "2025-05-30T10:18:42.256758Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "img = cv2.imread(train_image_path[1])\n",
    "img = cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n",
    "img = cv2.resize(img, (224, 224))\n",
    "plt.imshow(img)\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:18:42.716688Z",
     "iopub.status.busy": "2025-05-30T10:18:42.716265Z",
     "iopub.status.idle": "2025-05-30T10:18:42.722880Z",
     "shell.execute_reply": "2025-05-30T10:18:42.721521Z",
     "shell.execute_reply.started": "2025-05-30T10:18:42.716651Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hg8493lDLgT2"
   },
   "source": [
    "# Making Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:18:42.724577Z",
     "iopub.status.busy": "2025-05-30T10:18:42.724158Z",
     "iopub.status.idle": "2025-05-30T10:18:44.325913Z",
     "shell.execute_reply": "2025-05-30T10:18:44.324720Z",
     "shell.execute_reply.started": "2025-05-30T10:18:42.724541Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:41:40.483969Z",
     "iopub.status.busy": "2025-05-30T10:41:40.483572Z",
     "iopub.status.idle": "2025-05-30T10:41:40.496407Z",
     "shell.execute_reply": "2025-05-30T10:41:40.495167Z",
     "shell.execute_reply.started": "2025-05-30T10:41:40.483910Z"
    },
    "id": "KBlR_NG7LgT2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class ColorizationDataset(Dataset):\n",
    "    def __init__(self, paths, split='train'):\n",
    "        if split == 'train':\n",
    "            self.transforms = A.Compose([\n",
    "                A.Resize(image_size, image_size),\n",
    "                A.HorizontalFlip(p=0.4),\n",
    "                A.VerticalFlip(p=0.4),\n",
    "                A.RandomRotate90(p=0.5),\n",
    "                A.RandomBrightnessContrast(p=0.2),\n",
    "                A.RandomGamma (gamma_limit=(70, 130), p=0.2),\n",
    "            ])\n",
    "        elif split == 'val':\n",
    "            self.transforms = A.Compose([\n",
    "                A.Resize(image_size, image_size)\n",
    "            ])\n",
    "\n",
    "        self.split = split\n",
    "        self.size = image_size\n",
    "        self.paths = paths\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = Image.open(self.paths[idx]).convert(\"RGB\")\n",
    "        img = np.array(img)\n",
    "        augmented = self.transforms(image = img)\n",
    "        img = augmented['image']\n",
    "        \n",
    "        img_lab = rgb2lab(img).astype(\"float32\") # Converting RGB to L*a*b\n",
    "        img_lab = transforms.ToTensor()(img_lab)\n",
    "        L = img_lab[[0], ...] / 50. - 1. # Between -1 and 1\n",
    "        ab = img_lab[[1, 2], ...] / 110. # Between -1 and 1\n",
    "\n",
    "        return {'L': L, 'ab': ab}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "def make_dataloaders(batch_size=16, n_workers=4, pin_memory=True, **kwargs): # A handy function to make our dataloaders\n",
    "    dataset = ColorizationDataset(**kwargs)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=n_workers,\n",
    "                            pin_memory=pin_memory)\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:41:42.576483Z",
     "iopub.status.busy": "2025-05-30T10:41:42.576113Z",
     "iopub.status.idle": "2025-05-30T10:41:43.708392Z",
     "shell.execute_reply": "2025-05-30T10:41:43.707144Z",
     "shell.execute_reply.started": "2025-05-30T10:41:42.576457Z"
    },
    "id": "mGRut9WhLgT2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_dl = make_dataloaders(batch_size = batch_size, paths=train_image_path, split='train')\n",
    "val_dl = make_dataloaders(batch_size = batch_size, paths=val_image_path, split='val')\n",
    "\n",
    "data = next(iter(train_dl))\n",
    "Ls, abs_ = data['L'], data['ab']\n",
    "print(Ls.shape, abs_.shape)\n",
    "print(len(train_dl), len(val_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:41:49.290496Z",
     "iopub.status.busy": "2025-05-30T10:41:49.289342Z",
     "iopub.status.idle": "2025-05-30T10:41:49.304583Z",
     "shell.execute_reply": "2025-05-30T10:41:49.303525Z",
     "shell.execute_reply.started": "2025-05-30T10:41:49.290416Z"
    },
    "id": "ognNDeQWLgT2",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, nf, ni, submodule=None, input_c=None, dropout=False,\n",
    "                 innermost=False, outermost=False):\n",
    "        super().__init__()\n",
    "        self.outermost = outermost\n",
    "        if input_c is None: input_c = nf\n",
    "        downconv = nn.Conv2d(input_c, ni, kernel_size=4,\n",
    "                             stride=2, padding=1, bias=False)\n",
    "        downrelu = nn.LeakyReLU(0.2, True)\n",
    "        downnorm = nn.BatchNorm2d(ni)\n",
    "        uprelu = nn.ReLU(True)\n",
    "        upnorm = nn.BatchNorm2d(nf)\n",
    "\n",
    "        if outermost:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1)\n",
    "            down = [downconv]\n",
    "            up = [uprelu, upconv, nn.Tanh()]\n",
    "            model = down + [submodule] + up\n",
    "        elif innermost:\n",
    "            upconv = nn.ConvTranspose2d(ni, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            model = down + up\n",
    "        else:\n",
    "            upconv = nn.ConvTranspose2d(ni * 2, nf, kernel_size=4,\n",
    "                                        stride=2, padding=1, bias=False)\n",
    "            down = [downrelu, downconv, downnorm]\n",
    "            up = [uprelu, upconv, upnorm]\n",
    "            if dropout: up += [nn.Dropout(0.5)]\n",
    "            model = down + [submodule] + up\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.outermost:\n",
    "            return self.model(x)\n",
    "        else:\n",
    "            return torch.cat([x, self.model(x)], 1)\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    def __init__(self, input_c=1, output_c=2, n_down=8, num_filters=64):\n",
    "        super().__init__()\n",
    "        unet_block = UnetBlock(num_filters * 8, num_filters * 8, innermost=True)\n",
    "        for _ in range(n_down - 5):\n",
    "            unet_block = UnetBlock(num_filters * 8, num_filters * 8, submodule=unet_block, dropout=True)\n",
    "        out_filters = num_filters * 8\n",
    "        for _ in range(3):\n",
    "            unet_block = UnetBlock(out_filters // 2, out_filters, submodule=unet_block)\n",
    "            out_filters //= 2\n",
    "        self.model = UnetBlock(output_c, out_filters, input_c=input_c, submodule=unet_block, outermost=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uAS7A4-WLgT3"
   },
   "source": [
    "### Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:41:56.919850Z",
     "iopub.status.busy": "2025-05-30T10:41:56.919541Z",
     "iopub.status.idle": "2025-05-30T10:41:56.929566Z",
     "shell.execute_reply": "2025-05-30T10:41:56.928342Z",
     "shell.execute_reply.started": "2025-05-30T10:41:56.919830Z"
    },
    "id": "9w4OFLjtLgT_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class PatchDiscriminator(nn.Module):\n",
    "    def __init__(self, input_c, num_filters=64, n_down=3):\n",
    "        super().__init__()\n",
    "        model = [self.get_layers(input_c, num_filters, norm=False)]\n",
    "        model += [self.get_layers(num_filters * 2 ** i, num_filters * 2 ** (i + 1), s=1 if i == (n_down-1) else 2)\n",
    "                          for i in range(n_down)] # the 'if' statement is taking care of not using\n",
    "                                                  # stride of 2 for the last block in this loop\n",
    "        model += [self.get_layers(num_filters * 2 ** n_down, 1, s=1, norm=False, act=False)] # Make sure to not use normalization or\n",
    "                                                                                             # activation for the last layer of the model\n",
    "        self.model = nn.Sequential(*model)\n",
    "\n",
    "    def get_layers(self, ni, nf, k=4, s=2, p=1, norm=True, act=True): # when needing to make some repeatitive blocks of layers,\n",
    "        layers = [nn.Conv2d(ni, nf, k, s, p, bias=not norm)]          # it's always helpful to make a separate method for that purpose\n",
    "        if norm: layers += [nn.BatchNorm2d(nf)]\n",
    "        if act: layers += [nn.LeakyReLU(0.2, True)]\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vXkiUDX4LgT_"
   },
   "source": [
    " blocks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:41:59.425457Z",
     "iopub.status.busy": "2025-05-30T10:41:59.425112Z",
     "iopub.status.idle": "2025-05-30T10:41:59.472910Z",
     "shell.execute_reply": "2025-05-30T10:41:59.471911Z",
     "shell.execute_reply.started": "2025-05-30T10:41:59.425416Z"
    },
    "id": "lx0yDnIcLgT_",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PatchDiscriminator(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o8QmFohnLgUA"
   },
   "source": [
    " output shape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:42:02.827116Z",
     "iopub.status.busy": "2025-05-30T10:42:02.826152Z",
     "iopub.status.idle": "2025-05-30T10:42:04.275248Z",
     "shell.execute_reply": "2025-05-30T10:42:04.274149Z",
     "shell.execute_reply.started": "2025-05-30T10:42:02.827080Z"
    },
    "id": "xUSFECCQLgUA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "discriminator = PatchDiscriminator(3)\n",
    "dummy_input = torch.randn(16, 3, 256, 256) # batch_size, channels, size, size\n",
    "out = discriminator(dummy_input)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JkZqOAH0LgUA"
   },
   "source": [
    "### 1.5- GAN Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:42:09.992914Z",
     "iopub.status.busy": "2025-05-30T10:42:09.992611Z",
     "iopub.status.idle": "2025-05-30T10:42:10.001020Z",
     "shell.execute_reply": "2025-05-30T10:42:09.999886Z",
     "shell.execute_reply.started": "2025-05-30T10:42:09.992894Z"
    },
    "id": "fUv2yqAALgUA",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class GANLoss(nn.Module):\n",
    "    def __init__(self, gan_mode='vanilla', real_label=1.0, fake_label=0.0):\n",
    "        super().__init__()\n",
    "        self.register_buffer('real_label', torch.tensor(real_label))\n",
    "        self.register_buffer('fake_label', torch.tensor(fake_label))\n",
    "        if gan_mode == 'vanilla':\n",
    "            self.loss = nn.BCEWithLogitsLoss()\n",
    "        elif gan_mode == 'lsgan':\n",
    "            self.loss = nn.MSELoss()\n",
    "\n",
    "    def get_labels(self, preds, target_is_real):\n",
    "        if target_is_real:\n",
    "            labels = self.real_label\n",
    "        else:\n",
    "            labels = self.fake_label\n",
    "        return labels.expand_as(preds)\n",
    "\n",
    "    def __call__(self, preds, target_is_real):\n",
    "        labels = self.get_labels(preds, target_is_real)\n",
    "        loss = self.loss(preds, labels)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbR_dHMLLgUD"
   },
   "source": [
    "### Model Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:42:13.092841Z",
     "iopub.status.busy": "2025-05-30T10:42:13.092499Z",
     "iopub.status.idle": "2025-05-30T10:42:13.101565Z",
     "shell.execute_reply": "2025-05-30T10:42:13.100420Z",
     "shell.execute_reply.started": "2025-05-30T10:42:13.092817Z"
    },
    "id": "QDNgACH4LgUD",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def init_weights(net, init='norm', gain=0.02):\n",
    "\n",
    "    def init_func(m):\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and 'Conv' in classname:\n",
    "            if init == 'norm':\n",
    "                nn.init.normal_(m.weight.data, mean=0.0, std=gain)\n",
    "            elif init == 'xavier':\n",
    "                nn.init.xavier_normal_(m.weight.data, gain=gain)\n",
    "            elif init == 'kaiming':\n",
    "                nn.init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                nn.init.constant_(m.bias.data, 0.0)\n",
    "        elif 'BatchNorm2d' in classname:\n",
    "            nn.init.normal_(m.weight.data, 1., gain)\n",
    "            nn.init.constant_(m.bias.data, 0.)\n",
    "\n",
    "    net.apply(init_func)\n",
    "    print(f\"model initialized with {init} initialization\")\n",
    "    return net\n",
    "\n",
    "def init_model(model, device):\n",
    "    model = model.to(device)\n",
    "    model = init_weights(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:42:15.913861Z",
     "iopub.status.busy": "2025-05-30T10:42:15.913049Z",
     "iopub.status.idle": "2025-05-30T10:42:15.928582Z",
     "shell.execute_reply": "2025-05-30T10:42:15.927628Z",
     "shell.execute_reply.started": "2025-05-30T10:42:15.913828Z"
    },
    "id": "4aY1X4T1LgUE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class MainModel(nn.Module):\n",
    "    def __init__(self, device, net_G=None, lr_G=2e-4, lr_D=2e-4,\n",
    "                 beta1=0.5, beta2=0.999, lambda_L1=100., weight_decay=1e-5):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = device\n",
    "        self.lambda_L1 = lambda_L1\n",
    "\n",
    "        if net_G is None:\n",
    "            self.net_G = init_model(Unet(input_c=1, output_c=2, n_down=8, num_filters=64), self.device)\n",
    "        else:\n",
    "            self.net_G = net_G.to(self.device)\n",
    "        self.net_D = init_model(PatchDiscriminator(input_c=3, n_down=3, num_filters=64), self.device)\n",
    "        self.GANcriterion = GANLoss(gan_mode='vanilla').to(self.device)\n",
    "        self.L1criterion = nn.L1Loss()\n",
    "        self.opt_G = optim.Adam(self.net_G.parameters(), lr=lr_G, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "        self.opt_D = optim.Adam(self.net_D.parameters(), lr=lr_D, betas=(beta1, beta2), weight_decay=weight_decay)\n",
    "\n",
    "    def set_requires_grad(self, model, requires_grad=True):\n",
    "        for p in model.parameters():\n",
    "            p.requires_grad = requires_grad\n",
    "\n",
    "    def setup_input(self, data):\n",
    "        self.L = data['L'].to(self.device)\n",
    "        self.ab = data['ab'].to(self.device)\n",
    "\n",
    "    def forward(self):\n",
    "        self.fake_color = self.net_G(self.L)\n",
    "\n",
    "    def backward_D(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image.detach())\n",
    "        self.loss_D_fake = self.GANcriterion(fake_preds, False)\n",
    "        real_image = torch.cat([self.L, self.ab], dim=1)\n",
    "        real_preds = self.net_D(real_image)\n",
    "        self.loss_D_real = self.GANcriterion(real_preds, True)\n",
    "        self.loss_D = (self.loss_D_fake + self.loss_D_real) * 0.5\n",
    "        self.loss_D.backward()\n",
    "\n",
    "    def backward_G(self):\n",
    "        fake_image = torch.cat([self.L, self.fake_color], dim=1)\n",
    "        fake_preds = self.net_D(fake_image)\n",
    "        self.loss_G_GAN = self.GANcriterion(fake_preds, True)\n",
    "        self.loss_G_L1 = self.L1criterion(self.fake_color, self.ab) * self.lambda_L1\n",
    "        self.loss_G = self.loss_G_GAN + self.loss_G_L1\n",
    "        self.loss_G.backward()\n",
    "\n",
    "    def optimize(self):\n",
    "        self.forward()\n",
    "        self.net_D.train()\n",
    "        self.set_requires_grad(self.net_D, True)\n",
    "        self.opt_D.zero_grad()\n",
    "        self.backward_D()\n",
    "        self.opt_D.step()\n",
    "\n",
    "        self.net_G.train()\n",
    "        self.set_requires_grad(self.net_D, False)\n",
    "        self.opt_G.zero_grad()\n",
    "        self.backward_G()\n",
    "        self.opt_G.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:42:18.736579Z",
     "iopub.status.busy": "2025-05-30T10:42:18.736139Z",
     "iopub.status.idle": "2025-05-30T10:42:18.743474Z",
     "shell.execute_reply": "2025-05-30T10:42:18.742371Z",
     "shell.execute_reply.started": "2025-05-30T10:42:18.736548Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def create_model(device):\n",
    "    model = MainModel(device)\n",
    "    model = nn.DataParallel(model, device_ids=[0, 1])\n",
    "    model = model.to(device)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-30T10:42:20.506950Z",
     "iopub.status.busy": "2025-05-30T10:42:20.506616Z",
     "iopub.status.idle": "2025-05-30T10:43:41.669997Z",
     "shell.execute_reply": "2025-05-30T10:43:41.669001Z",
     "shell.execute_reply.started": "2025-05-30T10:42:20.506925Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model = create_model(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YB7zHB7aLgUE"
   },
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.647051Z",
     "iopub.status.idle": "2025-05-30T10:18:44.647417Z",
     "shell.execute_reply": "2025-05-30T10:18:44.647281Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.647267Z"
    },
    "id": "UbTgCD5zLgUE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.count, self.avg, self.sum = [0.] * 3\n",
    "\n",
    "    def update(self, val, count=1):\n",
    "        self.count += count\n",
    "        self.sum += count * val\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def create_loss_meters():\n",
    "    loss_D_fake = AverageMeter()\n",
    "    loss_D_real = AverageMeter()\n",
    "    loss_D = AverageMeter()\n",
    "    loss_G_GAN = AverageMeter()\n",
    "    loss_G_L1 = AverageMeter()\n",
    "    loss_G = AverageMeter()\n",
    "\n",
    "    return {'loss_D_fake': loss_D_fake,\n",
    "            'loss_D_real': loss_D_real,\n",
    "            'loss_D': loss_D,\n",
    "            'loss_G_GAN': loss_G_GAN,\n",
    "            'loss_G_L1': loss_G_L1,\n",
    "            'loss_G': loss_G}\n",
    "\n",
    "def update_losses(model, loss_meter_dict, count):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        loss = getattr(model, loss_name)\n",
    "        loss_meter.update(loss.item(), count=count)\n",
    "\n",
    "def lab_to_rgb(L, ab):\n",
    "    \"\"\"\n",
    "    Takes a batch of images\n",
    "    \"\"\"\n",
    "\n",
    "    L = (L + 1.) * 50.\n",
    "    ab = ab * 110.\n",
    "    Lab = torch.cat([L, ab], dim=1).permute(0, 2, 3, 1).cpu().numpy()\n",
    "    rgb_imgs = []\n",
    "    for img in Lab:\n",
    "        img_rgb = lab2rgb(img)\n",
    "        rgb_imgs.append(img_rgb)\n",
    "    return np.stack(rgb_imgs, axis=0)\n",
    "\n",
    "def visualize(model, data, save=True):\n",
    "    model.net_G.eval()\n",
    "    with torch.no_grad():\n",
    "        model.setup_input(data)\n",
    "        model.forward()\n",
    "    model.net_G.train()\n",
    "    fake_color = model.fake_color.detach()\n",
    "    real_color = model.ab\n",
    "    L = model.L\n",
    "    fake_imgs = lab_to_rgb(L, fake_color)\n",
    "    real_imgs = lab_to_rgb(L, real_color)\n",
    "    fig = plt.figure(figsize=(15, 8))\n",
    "    for i in range(5):\n",
    "        ax = plt.subplot(3, 5, i + 1)\n",
    "        ax.imshow(L[i][0].cpu(), cmap='gray')\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 5)\n",
    "        ax.imshow(fake_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "        ax = plt.subplot(3, 5, i + 1 + 10)\n",
    "        ax.imshow(real_imgs[i])\n",
    "        ax.axis(\"off\")\n",
    "    plt.show()\n",
    "    if save:\n",
    "        fig.savefig(f\"colorization_{time.time()}.png\")\n",
    "\n",
    "def log_results(loss_meter_dict):\n",
    "    for loss_name, loss_meter in loss_meter_dict.items():\n",
    "        print(f\"{loss_name}: {loss_meter.avg:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rfuk3_WNLgUF"
   },
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.648196Z",
     "iopub.status.idle": "2025-05-30T10:18:44.648605Z",
     "shell.execute_reply": "2025-05-30T10:18:44.648453Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.648412Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# learning_rate = \n",
    "\n",
    "epochs = 50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.649336Z",
     "iopub.status.idle": "2025-05-30T10:18:44.649797Z",
     "shell.execute_reply": "2025-05-30T10:18:44.649609Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.649591Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.651488Z",
     "iopub.status.idle": "2025-05-30T10:18:44.651851Z",
     "shell.execute_reply": "2025-05-30T10:18:44.651709Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.651692Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "PROJECT = \"Colorizing\"\n",
    "RESUME = \"allow\"\n",
    "WANDB_KEY = \"d9d14819dddd8a35a353b5c0b087e0f60d717140\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.653334Z",
     "iopub.status.idle": "2025-05-30T10:18:44.653686Z",
     "shell.execute_reply": "2025-05-30T10:18:44.653554Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.653540Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.login(\n",
    "    key = WANDB_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.655537Z",
     "iopub.status.idle": "2025-05-30T10:18:44.655879Z",
     "shell.execute_reply": "2025-05-30T10:18:44.655749Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.655735Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "     project=PROJECT,\n",
    "     resume=RESUME,\n",
    "     name=\"GanColorization_init\",\n",
    "     config={\n",
    "         \"epochs\": epochs,\n",
    "         \"batch_size\": batch_size,\n",
    "     },\n",
    " )\n",
    "wandb.watch(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.657479Z",
     "iopub.status.idle": "2025-05-30T10:18:44.657955Z",
     "shell.execute_reply": "2025-05-30T10:18:44.657737Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.657719Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=5, delta=0, save_path=\"best_model.pth\"):\n",
    "        self.patience = patience\n",
    "        self.delta = delta\n",
    "        self.counter = 0\n",
    "        self.best_loss = float('inf')\n",
    "        self.early_stop = False\n",
    "        self.save_path = save_path\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            print(f\"Validation loss improved to {val_loss:.5f}. Saving model to {self.save_path}\")\n",
    "            torch.save(model.state_dict(), self.save_path)  # Save the best model\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.661765Z",
     "iopub.status.idle": "2025-05-30T10:18:44.662172Z",
     "shell.execute_reply": "2025-05-30T10:18:44.661989Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.661975Z"
    },
    "id": "L6JVsE3ILgUF",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from skimage.metrics import peak_signal_noise_ratio as psnr, structural_similarity as ssim\n",
    "\n",
    "def validate(model, val_dl, loss_meter_dict):\n",
    "    model.eval()\n",
    "    psnr_vals = []\n",
    "    ssim_vals = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(val_dl, desc=\"Validation\", leave=False):\n",
    "            model.module.setup_input(data)\n",
    "            model.module.forward()\n",
    "            update_losses(model.module, loss_meter_dict, count=data['L'].size(0))\n",
    "            \n",
    "            fake_color = model.module.fake_color.detach()\n",
    "            real_color = model.module.ab\n",
    "            L = model.module.L\n",
    "\n",
    "            # 3) Convert Lab → RGB.  lab_to_rgb returns a NumPy array of shape (B, H, W, 3) with values in [0,1].\n",
    "            fake_imgs = lab_to_rgb(L, fake_color)\n",
    "            real_imgs = lab_to_rgb(L, real_color)  # np.ndarray (B, H, W, 3)\n",
    "\n",
    "            # 4) Compute PSNR & SSIM per-item\n",
    "            for i in range(fake_imgs.shape[0]):\n",
    "                real_np = real_imgs[i]  # already (H, W, 3), dtype float in [0,1]\n",
    "                fake_np = fake_imgs[i]\n",
    "\n",
    "                # PSNR in [0, 1]-space:\n",
    "                psnr_vals.append(psnr(real_np, fake_np, data_range=2.0))\n",
    "\n",
    "                # SSIM: use channel_axis=2 instead of multichannel=True\n",
    "                ssim_vals.append(\n",
    "                    ssim(\n",
    "                        real_np,\n",
    "                        fake_np,\n",
    "                        data_range=2.0,\n",
    "                        channel_axis=2,\n",
    "                        #win_size=win_size,  # ensure your H, W ≥ win_size,\n",
    "                                            # otherwise pick a smaller odd integer.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "    mean_psnr = float(np.mean(psnr_vals)) if psnr_vals else 0.0\n",
    "    mean_ssim = float(np.mean(ssim_vals)) if ssim_vals else 0.0\n",
    "    return mean_psnr, mean_ssim\n",
    "\n",
    "def train(model, train_dl, loss_meter_dict, display_every=3):\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for data in tqdm(train_dl, desc=\"Training\", leave=False):\n",
    "        model.module.setup_input(data)\n",
    "        model.module.optimize()\n",
    "        update_losses(model.module, loss_meter_dict, count=data['L'].size(0))\n",
    "        i += 1\n",
    "        if i % display_every == 0:\n",
    "            log_results(loss_meter_dict)    \n",
    "\n",
    "def train_model(model, train_dl, val_dl, epochs, display_every=1, patience=5, save_dir=\"models\"):\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    early_stopping = EarlyStopping(patience=patience, save_path=os.path.join(save_dir, \"best_model.pth\"))\n",
    "    schedulerG = torch.optim.lr_scheduler.ReduceLROnPlateau(model.module.opt_G, mode='min', patience=3)\n",
    "    # schedulerD = torch.optim.lr_scheduler.ReduceLROnPlateau(model.module.opt_D, mode='min', patience=3)\n",
    "\n",
    "    for e in range(epochs):\n",
    "        print(f\"\\nEpoch {e+1}/{epochs}\")\n",
    "        train_loss_meters = create_loss_meters()\n",
    "        train(model, train_dl, train_loss_meters, display_every)\n",
    "        print(\"Training Losses:\")\n",
    "        log_results(train_loss_meters)\n",
    "\n",
    "        val_loss_meters = create_loss_meters()\n",
    "        val_psnr, val_ssim = validate(model, val_dl, val_loss_meters)\n",
    "        print(\"Validation Losses:\")\n",
    "        log_results(val_loss_meters)\n",
    "\n",
    "        val_loss = val_loss_meters[\"loss_G\"].avg  # Use loss_G as validation loss for early stopping and scheduler\n",
    "        schedulerG.step(val_loss)\n",
    "\n",
    "        wandb.log({\n",
    "            \"train/loss_D\": train_loss_meters[\"loss_D\"].avg,\n",
    "            \"train/loss_G\": train_loss_meters[\"loss_G\"].avg,\n",
    "            \"val/loss_D\": val_loss_meters[\"loss_D\"].avg,\n",
    "            \"val/loss_G\": val_loss_meters[\"loss_G\"].avg,\n",
    "            \"val/PSNR\": val_psnr,\n",
    "            \"val/SSIM\": val_ssim\n",
    "        })\n",
    "        print(f\"Validation PSNR: {val_psnr:.4f}, SSIM: {val_ssim:.4f}\")\n",
    "\n",
    "        # Check early stopping\n",
    "        early_stopping(val_loss, model.module)\n",
    "        if early_stopping.early_stop:\n",
    "            print(\"Early stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Save the model at the last epoch\n",
    "    torch.save(model.module.state_dict(), os.path.join(save_dir, \"last_epoch_model.pth\"))\n",
    "    print(\"Model saved at last epoch.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-05-30T10:18:44.665024Z",
     "iopub.status.idle": "2025-05-30T10:18:44.665526Z",
     "shell.execute_reply": "2025-05-30T10:18:44.665301Z",
     "shell.execute_reply.started": "2025-05-30T10:18:44.665280Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "train_model(model, train_dl, val_dl, epochs, 100, save_dir = '/kaggle/working/')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 857191,
     "sourceId": 1462296,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
